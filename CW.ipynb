{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CW.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VictoriaDraganova/Dataset-Condensation-with-Gradient-Matching/blob/main/CW.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1JkFJ7alk5ti"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yn9E_oHiMsw"
      },
      "source": [
        "# Execute this code block to install dependencies when running on colab\n",
        "try:\n",
        "    import torch\n",
        "except:\n",
        "    from os.path import exists\n",
        "    from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "    platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "    cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
        "    accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
        "\n",
        "    !pip install -q http://download.pytorch.org/whl/{accelerator}/torch-1.0.0-{platform}-linux_x86_64.whl torchvision\n",
        "\n",
        "try: \n",
        "    import torchbearer\n",
        "except:\n",
        "    !pip install torchbearer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6wJC2hIkk36"
      },
      "source": [
        "# automatically reload external modules if they change\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "import torchbearer\n",
        "import tqdm.notebook as tq\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import MNIST\n",
        "from torchbearer import Trial\n",
        "import numpy as np\n",
        "import copy\n",
        "from torch.utils.data import Dataset\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "from torchvision.utils import save_image\n",
        "import os\n",
        "import statistics as st\n",
        "import os.path\n",
        "from os import path\n",
        "import math"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTbEj4kpcPaA"
      },
      "source": [
        "n_classes = 0\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "channel = 1\n",
        "im_size = []\n",
        "trainset = []\n",
        "testset = []\n",
        "trainset_copy = []\n",
        "images_all = []\n",
        "labels_all = []\n",
        "indices_class = []"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bisvwpmoi2kf"
      },
      "source": [
        "class Synthetic(Dataset):\n",
        "    def __init__(self, data, targets):\n",
        "        self.data = data.detach().float()\n",
        "        self.targets = targets.detach()\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.data[index], self.targets[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data.shape[0]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7WPOQUsgqBTg"
      },
      "source": [
        "def sample_batch(data):\n",
        "  batches = DataLoader(data, batch_size=256, shuffle=True)\n",
        "  for data, target in (batches):\n",
        "    data, target = data.to(device), target.to(device)\n",
        "    return data, target"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGUESGdNqYjz"
      },
      "source": [
        "def updateNetwork(optimizer, steps, loss_function, net, syn_data_data, syn_data_target):\n",
        "  for s in range(steps):\n",
        "    net.train()\n",
        "    prediction_syn = net(syn_data_data)\n",
        "    loss_syn = loss_function(prediction_syn, syn_data_target)\n",
        "    optimizer.zero_grad()\n",
        "    loss_syn.backward()\n",
        "    optimizer.step()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_rNEdLuIM5zp"
      },
      "source": [
        "#based on author's published code\n",
        "def distance(grad1, grad2):\n",
        "  dist = torch.tensor(0.0).to(device)\n",
        "  for gr, gs in zip(grad1, grad2):\n",
        "    shape=gr.shape\n",
        "    if len(shape) == 4: \n",
        "        gr = gr.reshape(shape[0], shape[1] * shape[2] * shape[3])\n",
        "        gs = gs.reshape(shape[0], shape[1] * shape[2] * shape[3])\n",
        "    elif len(shape) == 3:  \n",
        "        gr = gr.reshape(shape[0], shape[1] * shape[2])\n",
        "        gs = gs.reshape(shape[0], shape[1] * shape[2])\n",
        "    elif len(shape) == 2: \n",
        "        tmp = 'do nothing'\n",
        "    elif len(shape) == 1: \n",
        "        gr = gr.reshape(1, shape[0])\n",
        "        gs = gs.reshape(1, shape[0])\n",
        "        continue\n",
        "    dis_weight = torch.sum(1 - torch.sum(gr * gs, dim=-1) / (torch.norm(gr, dim=-1) * torch.norm(gs, dim=-1)+ 0.000001))\n",
        "    dist+=dis_weight\n",
        "  return dist"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Ar7kZSGq86G"
      },
      "source": [
        "#from author's published code\n",
        "def get_images(c, n): # get random n images from class c\n",
        "    idx_shuffle = np.random.permutation(indices_class[c])[:n]\n",
        "    return images_all[idx_shuffle]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCMj55NNYP18"
      },
      "source": [
        "#create synthetic data\n",
        "def train_synthetic(model, dataset, images_per_class,  iterations, network_steps):\n",
        "  synthetic_datas = []\n",
        "  T = images_per_class\n",
        "  K = iterations\n",
        "  for i in range(1): #to generate 1 synthetic datasets\n",
        "    #create synthetic data\n",
        "    data_syn = torch.randn(size=(n_classes*T, channel, im_size[0], im_size[1]), dtype=torch.float, requires_grad=True, device=device)\n",
        "    targets_syn = torch.tensor([np.ones(T)*i for i in range(n_classes)], dtype=torch.long, requires_grad=False,  device=device).view(-1) \n",
        "\n",
        "    #optimizer for image\n",
        "    optimizer_img = torch.optim.SGD([data_syn, ], lr=0.1) # optimizer_img for synthetic data; only update synthetic image, labels don't change\n",
        "    optimizer_img.zero_grad()\n",
        "    loss_function = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "    #training synthetic data\n",
        "    for k in tq.tqdm(range(K)):\n",
        "      net = new_network(model).to(device)\n",
        "      net.train()\n",
        "      net_parameters = list(net.parameters())\n",
        "      optimizer_net = torch.optim.SGD(net.parameters(), lr=0.01)  # optimizer_net for network\n",
        "      optimizer_net.zero_grad()\n",
        "      loss_avg = 0\n",
        "      for t in range(T):\n",
        "        loss = torch.tensor(0.0).to(device)\n",
        "        for c in range(n_classes):\n",
        "          img_real = get_images(c, 256)\n",
        "          targets_real = torch.ones((img_real.shape[0],), device=device, dtype=torch.long) * c\n",
        "          prediction_real = net(img_real) # makes prediction\n",
        "          \n",
        "          loss_real = loss_function(prediction_real, targets_real) # computes the cross entropy loss\n",
        "          gw_real = torch.autograd.grad(loss_real, net_parameters) # returns the sum of the gradients of the loss wrt the network parameters\n",
        "\n",
        "          data_synth = data_syn[c*T:(c+1)*T].reshape((T, channel, im_size[0], im_size[1]))\n",
        "          targets_synth = torch.ones((T,), device=device, dtype=torch.long) * c\n",
        "          prediction_syn = net(data_synth)\n",
        "          loss_syn = loss_function(prediction_syn, targets_synth)\n",
        "          gw_syn = torch.autograd.grad(loss_syn, net_parameters, create_graph=True)\n",
        "\n",
        "          dist = distance(gw_syn, gw_real)\n",
        "          loss+=dist\n",
        "\n",
        "        optimizer_img.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer_img.step()\n",
        "        loss_avg += loss.item()\n",
        "\n",
        "        if t == T - 1:\n",
        "          break\n",
        "\n",
        "        updateNetwork(optimizer_net, network_steps, loss_function, net, data_syn, targets_syn)\n",
        "        \n",
        "      loss_avg /= (n_classes*T)\n",
        "      if k%10 == 0:\n",
        "            print('iter = %.4f, loss = %.4f' % (k, loss_avg))\n",
        "            # model_save_name = 'data_syn.pt'\n",
        "            # path = F\"/content/gdrive/MyDrive/{model_save_name}\"  #to save synthetic data\n",
        "            # torch.save(data_syn, path)\n",
        "    synthetic_datas.append(data_syn)\n",
        "\n",
        "    print('Synthetic %d created ' % (i))  \n",
        "\n",
        "  return synthetic_datas\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Up1gDvtlGOA"
      },
      "source": [
        "#evaluation of synthetic data produced\n",
        "def evaluation(model, all_synthetic_data, images_per_class):\n",
        "  accuracies = []\n",
        "  targets_syn = torch.tensor([np.ones(images_per_class)*i for i in range(n_classes)], dtype=torch.long, requires_grad=False,  device=device).view(-1) \n",
        "  for data in all_synthetic_data:\n",
        "    loss_function = nn.CrossEntropyLoss().to(device)\n",
        "    for it in range(20): #number of random models for evaluation\n",
        "      print(it)\n",
        "      net = new_network(model).to(device)\n",
        "      net.train()\n",
        "      net_parameters = list(net.parameters())\n",
        "      optimizer_train = torch.optim.SGD(net.parameters(), lr=0.01) \n",
        "      optimizer_train.zero_grad()\n",
        "      trial = Trial(net,optimizer=optimizer_train, criterion=loss_function, metrics=['loss', 'accuracy'], verbose=0).to(device)\n",
        "      syn_data_whole = Synthetic(data, targets_syn)\n",
        "      train_loader = DataLoader(syn_data_whole, batch_size=256, shuffle=True)\n",
        "      test_loader = DataLoader(testset, batch_size=256, shuffle=False)\n",
        "      trial.with_generators(train_loader, test_generator=test_loader)\n",
        "      trial.run(epochs=300)\n",
        "      results = trial.evaluate(data_key=torchbearer.TEST_DATA)\n",
        "      print()\n",
        "      print(results)\n",
        "      accuracies.append(results['test_acc'])\n",
        "  \n",
        "  average_acc = sum(accuracies)/len(accuracies)\n",
        "  std_acc = st.pstdev(accuracies)\n",
        "  print(\"Model is: \", model)\n",
        "  print(\"Standard deviation is : \" , std_acc)\n",
        "  print(\"Average is : \" ,average_acc)\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mP__NGGyaWhS"
      },
      "source": [
        "def createData(dataset):\n",
        "\n",
        "  global im_size\n",
        "  global trainset\n",
        "  global testset\n",
        "  global trainset_copy\n",
        "  global n_classes\n",
        "  global channel\n",
        "  global images_all\n",
        "  global labels_all\n",
        "  global indices_class\n",
        "\n",
        "  if dataset == \"MNIST\":\n",
        "    !wget https://artist-cloud.ecs.soton.ac.uk/s/sFkQ7HYOekDoDEG/download\n",
        "    !unzip download\n",
        "    !mv mnist MNIST\n",
        "    from torchvision.datasets import MNIST\n",
        "    mean = [0.1307]\n",
        "    std = [0.3015]\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize(mean=mean, std=std)\n",
        "    ])\n",
        "    trainset = MNIST(\".\", train=True, download=True, transform=transform)\n",
        "    testset = MNIST(\".\", train=False, download=True, transform=transform)\n",
        "    trainset_copy = MNIST(\".\", train=True, download=True, transform=transform)\n",
        "    n_classes = 10\n",
        "    channel = 1\n",
        "    im_size = [28,28]\n",
        "\n",
        "  elif dataset == \"FashionMNIST\":\n",
        "    from torchvision.datasets import FashionMNIST\n",
        "    mean = [0.2860]\n",
        "    std =  [0.3205]\n",
        "    transform = transforms.Compose([\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize(mean=mean, std=std)\n",
        "    ])\n",
        "    trainset = FashionMNIST(\".\", train=True, download=True, transform=transform)\n",
        "    testset = FashionMNIST(\".\", train=False, download=True, transform=transform)\n",
        "    trainset_copy = FashionMNIST(\".\", train=True, download=True, transform=transform)\n",
        "    n_classes = 10\n",
        "    channel = 1\n",
        "    im_size = [28,28]\n",
        "\n",
        "  elif dataset == \"SVHN\":\n",
        "    from torchvision.datasets import SVHN\n",
        "    mean = [0.4377, 0.4438, 0.4728]\n",
        "    std = [0.1201, 0.1231, 0.1052]\n",
        "    transform = transforms.Compose([\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize(mean=mean, std=std)\n",
        "    ])\n",
        "    trainset = SVHN(\".\", split='train', transform=transform, download=True)\n",
        "    testset = SVHN(\".\", split='test', transform=transform, download=True)\n",
        "    trainset_copy = SVHN(\".\", split='test', transform=transform, download=True)\n",
        "    n_classes = 10\n",
        "    channel = 3\n",
        "    im_size = [32,32]\n",
        "\n",
        "  elif dataset == \"CIFAR10\":\n",
        "    from torchvision.datasets import CIFAR10\n",
        "    mean = [0.4914, 0.4822, 0.4465]\n",
        "    std = [0.2023, 0.1994, 0.2010]\n",
        "    transform = transforms.Compose([\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize(mean=mean, std=std)\n",
        "    ])\n",
        "    trainset = CIFAR10(\".\", train=True, download=True, transform=transform)\n",
        "    testset = CIFAR10(\".\", train=False, download=True, transform=transform)\n",
        "    trainset_copy = CIFAR10(\".\", train=True, download=True, transform=transform)\n",
        "    n_classes = 10\n",
        "    channel = 3\n",
        "    im_size = [32,32]\n",
        "\n",
        "  #from author's published code\n",
        "  indices_class = [[] for c in range(n_classes)]\n",
        "  images_all = [torch.unsqueeze(trainset[i][0], dim=0) for i in range(len(trainset))]\n",
        "  labels_all = [trainset[i][1] for i in range(len(trainset))]\n",
        "  for i, lab in enumerate(labels_all):\n",
        "      indices_class[lab].append(i)\n",
        "  images_all = torch.cat(images_all, dim=0).to(device)\n",
        "  labels_all = torch.tensor(labels_all, dtype=torch.long, device=device)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXxVPf9fMETw"
      },
      "source": [
        "**Networks**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRQ06pr1J1gh"
      },
      "source": [
        "#to calculate image output size\n",
        "def calculate(size, kernel, stride, padding):\n",
        "  return int(((size+(2*padding)-kernel)/stride) + 1)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kt42YCvzCLBd"
      },
      "source": [
        "#based on https://cs231n.github.io/convolutional-networks/\n",
        "class CNN(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super(CNN, self).__init__()\n",
        "    outsize = im_size[0]\n",
        "    self.conv1 = nn.Conv2d(in_channels=channel, out_channels=128, kernel_size=3, padding=1) #32*32\n",
        "    outsize = calculate(outsize,3,1,1)\n",
        "    self.norm1 = nn.GroupNorm(128, 128)\n",
        "    self.avg_pooling1 = nn.AvgPool2d(kernel_size=2, stride=2) # (n+2p-f)/s+1 => 32+0-2/2 + 1 =16\n",
        "    outsize = calculate(outsize,2,2,0)\n",
        "    self.conv2 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1) #out = (n+2p-f)/s+1 => 16+2-3/1 + 1 => 16\n",
        "    outsize = calculate(outsize,3,1,1)\n",
        "    self.norm2 = nn.GroupNorm(128, 128)\n",
        "    self.avg_pooling2 = nn.AvgPool2d(kernel_size=2, stride=2) #out = (n+2p-f)/s+1 => 16+0-2/2 +1 => 8\n",
        "    outsize = calculate(outsize,2,2,0)\n",
        "    self.conv3 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1) #out = (n+2p-f)/s+1 => 8+2-3/1 + 1 => 8\n",
        "    outsize = calculate(outsize,3,1,1)\n",
        "    self.norm3 = nn.GroupNorm(128, 128)\n",
        "    self.avg_pooling3 = nn.AvgPool2d(kernel_size=2, stride=2) #out = (n+2p-f)/s+1 => 8+0-2/2 +1 => 4\n",
        "    outsize = calculate(outsize,2,2,0)\n",
        "    self.classifier = nn.Linear(outsize*outsize*128, 10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.conv1(x)\n",
        "    out = self.norm1(out)\n",
        "    out = F.relu(out)\n",
        "    out = self.avg_pooling1(out)\n",
        "    out = self.conv2(out)\n",
        "    out = self.norm2(out)\n",
        "    out = F.relu(out)\n",
        "    out = self.avg_pooling2(out)\n",
        "    out = self.conv3(out)\n",
        "    out = self.norm3(out)\n",
        "    out = F.relu(out)\n",
        "    out = self.avg_pooling3(out)\n",
        "    out = out.view(out.size(0), -1)\n",
        "    out = self.classifier(out)\n",
        "    return out"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aIDeOjd8Smni"
      },
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(im_size[0]*im_size[1]*channel, 128)\n",
        "        self.fc2 = nn.Linear(128, 128)\n",
        "        self.fc3 = nn.Linear(128, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = x.view(x.size(0), -1)\n",
        "        out = F.relu(self.fc1(out))\n",
        "        out = F.relu(self.fc2(out))\n",
        "        out = self.fc3(out)\n",
        "        return out"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxr6ahmXJsFV"
      },
      "source": [
        "#based on https://en.wikipedia.org/wiki/LeNet\n",
        "class LeNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet, self).__init__()\n",
        "        outsize = 28\n",
        "        self.conv1 = nn.Conv2d(channel, 6, kernel_size=5)\n",
        "        outsize = calculate(outsize, 5, 1,0)\n",
        "        self.avg1 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "        outsize = calculate(outsize, 2, 2,0)\n",
        "        self.conv2 = nn.Conv2d(6,16,kernel_size=5)\n",
        "        outsize = calculate(outsize, 5, 1,0)\n",
        "        self.avg2 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "        outsize = calculate(outsize, 2, 2,0)\n",
        "        self.fc1 = nn.Linear(outsize*outsize*16, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, n_classes)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = F.sigmoid(out)\n",
        "        out = self.avg1(out)\n",
        "        out = self.conv2(out)\n",
        "        out = F.sigmoid(out)\n",
        "        out = self.avg2(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = F.sigmoid(self.fc1(out))\n",
        "        out = F.sigmoid(self.fc2(out))\n",
        "        out = self.fc3(out)\n",
        "        return out"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUuDlPmhlvqS"
      },
      "source": [
        "trans = transforms.Resize((227,227))\n",
        "\n",
        "#based on https://www.analyticsvidhya.com/blog/2021/03/introduction-to-the-architecture-of-alexnet/ \n",
        "class AlexNet(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super(AlexNet, self).__init__()\n",
        "    outsize = 227\n",
        "    self.conv1 = nn.Conv2d(in_channels=channel, out_channels=96, kernel_size=11, padding=0, stride=4) #32*32\n",
        "    outsize = calculate(outsize,11,4,0)\n",
        "    self.max_pooling1 = nn.MaxPool2d(kernel_size=3, stride=2) # (n+2p-f)/s+1 => 32+0-2/2 + 1 =16\n",
        "    outsize = calculate(outsize,3,2,0)\n",
        "    self.conv2 = nn.Conv2d(in_channels=96, out_channels=256, kernel_size=5, padding=2, stride=1) #32*32\n",
        "    outsize = calculate(outsize,5,1,2)\n",
        "    self.max_pooling2 = nn.MaxPool2d(kernel_size=3, stride=2) # (n+2p-f)/s+1 => 32+0-2/2 + 1 =16\n",
        "    outsize = calculate(outsize,3,2,0)\n",
        "    self.conv3 = nn.Conv2d(in_channels=256, out_channels=384, kernel_size=3, padding=1, stride=1) #32*32\n",
        "    outsize = calculate(outsize,3,1,1)\n",
        "    self.conv4 = nn.Conv2d(in_channels=384, out_channels=384, kernel_size=3, padding=1, stride=1) #32*32\n",
        "    outsize = calculate(outsize,3,1,1)\n",
        "    self.conv5 = nn.Conv2d(in_channels=384, out_channels=256, kernel_size=3, padding=1, stride=1) #32*32\n",
        "    outsize = calculate(outsize,3,1,1)\n",
        "    self.max_pooling3 = nn.MaxPool2d(kernel_size=3, stride=2) # (n+2p-f)/s+1 => 32+0-2/2 + 1 =16\n",
        "    outsize = calculate(outsize,3,2,0)\n",
        "    self.dropout1 = nn.Dropout(p=0.5)\n",
        "    self.fc1 = nn.Linear(outsize*outsize*256, 4096)\n",
        "    self.dropout2 = nn.Dropout(p=0.5)\n",
        "    self.fc2 = nn.Linear(4096, 4096)\n",
        "    self.fc3 = nn.Linear(4096, 10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = trans(x)\n",
        "    out = self.conv1(x)\n",
        "    out = F.relu(out)\n",
        "    out = self.max_pooling1(out)\n",
        "    out = self.conv2(out)\n",
        "    out = F.relu(out)\n",
        "    out = self.max_pooling2(out)\n",
        "    out = self.conv3(out)\n",
        "    out = F.relu(out)\n",
        "    out = self.conv4(out)\n",
        "    out = F.relu(out)\n",
        "    out = self.conv5(out)\n",
        "    out = F.relu(out)\n",
        "    out = self.max_pooling3(out)\n",
        "    out = self.dropout1(out)\n",
        "    out = out.view(out.size(0), -1)\n",
        "    out = self.fc1(out)\n",
        "    out = F.relu(out)\n",
        "    out = self.dropout2(out)\n",
        "    out = self.fc2(out)\n",
        "    out = F.relu(out)\n",
        "    out = self.fc3(out)\n",
        "    out = F.softmax(out)\n",
        "    return out"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqynLzJiLDMf"
      },
      "source": [
        "#Author's published implementation of LeNet\n",
        "class LeNetTheirs(nn.Module):\n",
        "    def __init__(self, channel, num_classes):\n",
        "        super(LeNetTheirs, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(channel, 6, kernel_size=5, padding=2 if channel==1 else 0),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(6, 16, kernel_size=5),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "        self.fc_1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc_2 = nn.Linear(120, 84)\n",
        "        self.fc_3 = nn.Linear(84, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc_1(x))\n",
        "        x = F.relu(self.fc_2(x))\n",
        "        x = self.fc_3(x)\n",
        "        return x"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9k3wooaALELg"
      },
      "source": [
        "#Author's published implementation of AlexNet\n",
        "class AlexNetTheirs(nn.Module):\n",
        "    def __init__(self, channel, num_classes):\n",
        "        super(AlexNetTheirs, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(channel, 128, kernel_size=5, stride=1, padding=4 if channel==1 else 2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(128, 192, kernel_size=5, padding=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(192, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 192, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(192, 192, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "        self.fc = nn.Linear(192 * 4 * 4, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIM_8j0Gnnlm"
      },
      "source": [
        "#to get the model specified\n",
        "def new_network(model):\n",
        "  if model == \"CNN\":\n",
        "    return CNN()\n",
        "  if model == \"AlexNet\":\n",
        "    return AlexNet()\n",
        "  if model == \"AlexNetTheirs\":\n",
        "    return AlexNetTheirs(channel,n_classes)\n",
        "  if model == \"MLP\":\n",
        "    return MLP()\n",
        "  if model == \"LeNet\":\n",
        "    return LeNet()\n",
        "  if model == \"LeNetTheirs\":\n",
        "    return LeNetTheirs(channel,n_classes)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNaIndCwgJO3"
      },
      "source": [
        "# Experiment 1 \n",
        "def experiment1(model, dataset, images_per_class,  iterations, network_steps):\n",
        "  createData(dataset)\n",
        "  all_synthetic_datas = train_synthetic(model,dataset, images_per_class, iterations, network_steps)\n",
        "  evaluation(model,all_synthetic_datas, images_per_class)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4ajo2mJ9MvR"
      },
      "source": [
        "# Experiment 2\n",
        "def experiment2(model, dataset, images_per_class,  iterations, network_steps):\n",
        "  createData(dataset)\n",
        "  all_synthetic_datas = train_synthetic(model,dataset, images_per_class, iterations, network_steps)\n",
        "  models = [\"CNN\", \"MLP\", \"LeNet\", \"AlexNet\"] #models used to evaluate the synthetic data\n",
        "  for m in models:\n",
        "    evaluation(m, all_synthetic_datas, images_per_class)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZk0DFkLnHKx"
      },
      "source": [
        "experiment1(\"CNN\", \"SVHN\", 1, 1000, 1) #ConvNet model, MNIST dataset, 1000 iterations and 1 image per class"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UcLtka8A9PbA"
      },
      "source": [
        "experiment2(\"AlexNet\", \"MNIST\", 1, 1000, 1) #ConvNet model, MNIST dataset, 1000 iterations and 1 image per class"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwBa5LzxJ9A_"
      },
      "source": [
        "**For mean and std**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4scnkSBM6oJj"
      },
      "source": [
        "#To find the mean and std of the datasets\n",
        "transform = transforms.Compose([\n",
        "  transforms.ToTensor()\n",
        "])\n",
        "\n",
        "from torchvision.datasets import MNIST\n",
        "trainset = MNIST(\".\", train=True, download=True, transform=transform)\n",
        "testset = MNIST(\".\", train=False, download=True, transform=transform)\n",
        "trainset_copy = MNIST(\".\", train=True, download=True, transform=transform)\n",
        "\n",
        "# from torchvision.datasets import FashionMNIST\n",
        "# trainset = FashionMNIST(\".\", train=True, download=True, transform=transform)\n",
        "# testset = FashionMNIST(\".\", train=False, download=True, transform=transform)\n",
        "# trainset_copy = FashionMNIST(\".\", train=True, download=True, transform=transform)\n",
        "\n",
        "# from torchvision.datasets import CIFAR10\n",
        "# trainset = CIFAR10(\".\", train=True, download=True, transform=transform)\n",
        "# testset = CIFAR10(\".\", train=False, download=True, transform=transform)\n",
        "# trainset_copy = CIFAR10(\".\", train=True, download=True, transform=transform)\n",
        "\n",
        "# from torchvision.datasets import SVHN\n",
        "# trainset = SVHN(\".\", split='train', transform=transform, download=True)\n",
        "# testset = SVHN(\".\", split='test', transform=transform, download=True)\n",
        "# trainset_copy = SVHN(\".\", split='test', transform=transform, download=True)\n",
        "\n",
        "\n",
        "loader = DataLoader(trainset, batch_size=256, num_workers=0, shuffle=False)\n",
        "\n",
        "mean = 0.\n",
        "std = 0.\n",
        "for images, _ in loader:\n",
        "    batch_samples = images.size(0) \n",
        "    images = images.view(batch_samples, images.size(1), -1)\n",
        "    mean += images.mean(2).sum(0)\n",
        "    std += images.std(2).sum(0)\n",
        "\n",
        "mean /= len(loader.dataset)\n",
        "std /= len(loader.dataset)\n",
        "\n",
        "print(mean)\n",
        "print(std)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eS_lRGbb86ih"
      },
      "source": [
        "MNIST\n",
        "mean = [0.1307], std = [0.3015]\n",
        "\n",
        "FashionMNIST\n",
        "mean = [0.2860], std = [0.3205]\n",
        "\n",
        "SVHN\n",
        "mean = [0.4377, 0.4438, 0.4728],\n",
        "std = [0.1201, 0.1231, 0.1052]\n",
        "\n",
        "CIFAR10\n",
        "mean = [0.4914, 0.4822, 0.4465],\n",
        "std = [0.2023, 0.1994, 0.2010]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ox1gE4IJBbn_"
      },
      "source": [
        "If there is a crash, reload:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kD9w1frlJlp"
      },
      "source": [
        "path = F\"/content/gdrive/MyDrive/data_syn.pt\" \n",
        "syn=torch.load(path)\n",
        "print(syn.shape)\n",
        "all_synthetic_data=[]\n",
        "all_synthetic_data.append(syn)\n",
        "loss_function = nn.CrossEntropyLoss().to(device)\n",
        "#evaluation(\"CNN\",all_synthetic_data, 1)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}